{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    \n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "       \n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "            \n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "                \n",
        "    # update the network's weights and biases by applying gradient descent \n",
        "    # using backpropagation to a single mini batch.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) #* \\\n",
        "  #          sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    # the neural network's output is assumed to be the index of \n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "    \n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "    \n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "id": "lg7xJiLiBUSl",
        "outputId": "3c8bf47d-a682-4799-cf31-f6cfc4840f20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.2012548 ],\n",
              "        [ 0.34268377],\n",
              "        [-1.05331073]]),\n",
              " array([[0.05745517]])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "id": "rYzcbJs2CMQ-",
        "outputId": "e0c4e47b-6c01-4a83-b3c4-86f15fc70360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.80252056, -0.25635687],\n",
              "        [ 0.84055294,  0.05929157],\n",
              "        [-1.12097732, -1.38434122]]),\n",
              " array([[0.1591467 , 0.73770062, 0.50911051]])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "568ec6f4-f129-4476-8ab0-8afb9dde9da1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PHYS3151-Machine-Learning-in-Physics-2023'...\n",
            "remote: Enumerating objects: 556, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 556 (delta 75), reused 115 (delta 62), pack-reused 417\n",
            "Receiving objects: 100% (556/556), 29.64 MiB | 25.27 MiB/s, done.\n",
            "Resolving deltas: 100% (292/292), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[9].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "3e883812-2b9e-427b-905f-93b66b714a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbiklEQVR4nO3df2yV5f3/8dcpP44o7YFa2tNKwQICRoRtKLVDGY6O0i3Kryyg/gGLw4DFqUxdukzRzaQTE2c0DGfm6NwEkWVAJI5MCy37UXCghJi5jjbV1tCWgeEcKLaQ9vr+wdfz8UgL3odz+j49fT6SK+m57/vd683lbV/c59zc9TnnnAAA6GNp1g0AAAYmAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmBls38GXd3d06evSo0tPT5fP5rNsBAHjknNOpU6eUl5entLTer3OSLoCOHj2q/Px86zYAAJepublZo0eP7nV/0r0Fl56ebt0CACAOLvXzPGEBtH79el177bW64oorVFhYqHffffcr1fG2GwCkhkv9PE9IAG3ZskVr1qzR2rVr9d5772natGkqKSnRsWPHEjEdAKA/cgkwY8YMV1ZWFnnd1dXl8vLyXEVFxSVrQ6GQk8RgMBiMfj5CodBFf97H/Qro7NmzOnjwoIqLiyPb0tLSVFxcrNra2guO7+zsVDgcjhoAgNQX9wA6fvy4urq6lJOTE7U9JydHra2tFxxfUVGhQCAQGdwBBwADg/ldcOXl5QqFQpHR3Nxs3RIAoA/E/d8BZWVladCgQWpra4va3tbWpmAweMHxfr9ffr8/3m0AAJJc3K+Ahg4dqunTp6uqqiqyrbu7W1VVVSoqKor3dACAfiohT0JYs2aNli1bpptuukkzZszQ888/r/b2dv3gBz9IxHQAgH4oIQG0ZMkS/e9//9MTTzyh1tZWfe1rX9OuXbsuuDEBADBw+ZxzzrqJLwqHwwoEAtZtAAAuUygUUkZGRq/7ze+CAwAMTAQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMDHYugEASFYffvhhn8xz/fXX98k8yYYrIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GCmAlPf666/HVDdhwgTPNX/9619jmmsg4goIAGCCAAIAmIh7AD355JPy+XxRY/LkyfGeBgDQzyXkM6AbbrhB77zzzv9NMpiPmgAA0RKSDIMHD1YwGEzEtwYApIiEfAZ05MgR5eXlady4cbrnnnvU1NTU67GdnZ0Kh8NRAwCQ+uIeQIWFhaqsrNSuXbu0YcMGNTY26rbbbtOpU6d6PL6iokKBQCAy8vPz490SACAJxT2ASktL9f3vf19Tp05VSUmJ3nrrLZ08eVJvvPFGj8eXl5crFApFRnNzc7xbAgAkoYTfHTBixAhNnDhR9fX1Pe73+/3y+/2JbgMAkGQS/u+ATp8+rYaGBuXm5iZ6KgBAPxL3AHrkkUdUU1Ojjz76SP/85z+1cOFCDRo0SHfddVe8pwIA9GNxfwvuk08+0V133aUTJ05o1KhRuvXWW7Vv3z6NGjUq3lMBAPqxuAdQrA/9A4CvorKy0nPN4sWLY5qrq6vLc83OnTtjmmsg4llwAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATCT8F9IBQDwVFRV5rklLi+3v2v/9738912zYsCGmuQYiroAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GjbwBfPnz/dc8/TTT3uuuf322z3XHD9+3HNNslu9erXnmvz8fM81n376qecaSVqxYkVMdfhquAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwuecc9ZNfFE4HFYgELBuAwPUsWPHPNdkZWV5rrnzzjs91+zcudNzTbJraWnxXBMMBj3X/PCHP/RcI0mvvPJKTHU4LxQKKSMjo9f9XAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwMdi6ASCZdHZ2eq6J5Xm+V155peeaZDdz5kzPNSNHjvRcE8t6Dxs2zHMNEo8rIACACQIIAGDCcwDt3btXd9xxh/Ly8uTz+bR9+/ao/c45PfHEE8rNzdWwYcNUXFysI0eOxKtfAECK8BxA7e3tmjZtmtavX9/j/nXr1umFF17QSy+9pP379+uqq65SSUmJOjo6LrtZAEDq8HwTQmlpqUpLS3vc55zT888/r5/97GeaP3++JOnVV19VTk6Otm/frqVLl15etwCAlBHXz4AaGxvV2tqq4uLiyLZAIKDCwkLV1tb2WNPZ2alwOBw1AACpL64B1NraKknKycmJ2p6TkxPZ92UVFRUKBAKRkZ+fH8+WAABJyvwuuPLycoVCochobm62bgkA0AfiGkDBYFCS1NbWFrW9ra0tsu/L/H6/MjIyogYAIPXFNYAKCgoUDAZVVVUV2RYOh7V//34VFRXFcyoAQD/n+S6406dPq76+PvK6sbFRhw4dUmZmpsaMGaOHHnpITz/9tK677joVFBTo8ccfV15enhYsWBDPvgEA/ZznADpw4IBuv/32yOs1a9ZIkpYtW6bKyko99thjam9v13333aeTJ0/q1ltv1a5du3TFFVfEr2sAQL/nc7E82S+BwuGwAoGAdRvo515++eWY6u69917PNceOHfNc8/Wvf91zTW93kibC8OHDPdf85S9/8VzzzW9+03PNRx995Llm8uTJnmsk6dy5czHV4bxQKHTRz/XN74IDAAxMBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATnn8dA9DXCgoKPNfcfffdMc3V3d3tuWbVqlWea/ryydax2LJli+eaWJ5sffr0ac8148eP91yD5MQVEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABM8jBR9qrCw0HPNW2+95blm2LBhnmskaevWrZ5rtm/fHtNcfeGZZ56Jqa6kpCTOnfRs3bp1fTIPkhNXQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEz4nHPOuokvCofDCgQC1m0MKIMHx/ZM2gcffNBzzbPPPuu5xufzea6J9bT++OOPPdfE8jDS8vJyzzWjRo3yXPO3v/3Nc40kjR492nPNnj17PNd85zvf8VyD/iMUCikjI6PX/VwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMHDSBHTQ0Ul6bnnnotzJz2L5WGkJ06ciGmuzMzMmOq8ampq8lwTS2/Dhw/3XCNJZ86c8VyTnp4e01xIXTyMFACQlAggAIAJzwG0d+9e3XHHHcrLy5PP57vgd6EsX75cPp8vasybNy9e/QIAUoTnAGpvb9e0adO0fv36Xo+ZN2+eWlpaImPz5s2X1SQAIPV4/lWYpaWlKi0tvegxfr9fwWAw5qYAAKkvIZ8BVVdXKzs7W5MmTdKqVasuekdSZ2enwuFw1AAApL64B9C8efP06quvqqqqSs8884xqampUWlqqrq6uHo+vqKhQIBCIjPz8/Hi3BABIQp7fgruUpUuXRr6+8cYbNXXqVI0fP17V1dWaM2fOBceXl5drzZo1kdfhcJgQAoABIOG3YY8bN05ZWVmqr6/vcb/f71dGRkbUAACkvoQH0CeffKITJ04oNzc30VMBAPoRz2/BnT59OupqprGxUYcOHVJmZqYyMzP11FNPafHixQoGg2poaNBjjz2mCRMmqKSkJK6NAwD6N88BdODAAd1+++2R159/frNs2TJt2LBBhw8f1u9//3udPHlSeXl5mjt3rn7xi1/I7/fHr2sAQL/Hw0hTzP333++55sUXX4xpru7ubs81HR0dnmuWL1/uuaa1tdVzjST99re/9VwzceLEmObyKpaHsvbl/96nT5/2XHPLLbd4rvnwww8918AGDyMFACQlAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJnoadYmJ5UnBOTk5Mc1VUVHiuefbZZ2Oaq69Mnz7dc82WLVs81xQUFHiuSfanYe/du9dzzRd/tQtSD0/DBgAkJQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYGWzeA+PrTn/7kueZ3v/tdTHM1NjbGVJfMRo8e7bnmmmuuSUAnFyorK/Nc8+677yagk541NDT02VxIDVwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOFzzjnrJr4oHA4rEAhYt4F+buTIkTHVxfJg1vnz53uu+fTTTz3XZGVlea4BLIVCIWVkZPS6nysgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgZbNwAkwuOPPx5T3Z133um5pr293XPNTTfd5LkGSDVcAQEATBBAAAATngKooqJCN998s9LT05Wdna0FCxaorq4u6piOjg6VlZXp6quv1vDhw7V48WK1tbXFtWkAQP/nKYBqampUVlamffv26e2339a5c+c0d+7cqPfAH374Yb355pvaunWrampqdPToUS1atCjujQMA+jdPNyHs2rUr6nVlZaWys7N18OBBzZo1S6FQSK+88oo2bdqkb3/725KkjRs36vrrr9e+fft0yy23xK9zAEC/dlmfAYVCIUlSZmamJOngwYM6d+6ciouLI8dMnjxZY8aMUW1tbY/fo7OzU+FwOGoAAFJfzAHU3d2thx56SDNnztSUKVMkSa2trRo6dKhGjBgRdWxOTo5aW1t7/D4VFRUKBAKRkZ+fH2tLAIB+JOYAKisr0wcffKDXX3/9shooLy9XKBSKjObm5sv6fgCA/iGmf4i6evVq7dy5U3v37tXo0aMj24PBoM6ePauTJ09GXQW1tbUpGAz2+L38fr/8fn8sbQAA+jFPV0DOOa1evVrbtm3T7t27VVBQELV/+vTpGjJkiKqqqiLb6urq1NTUpKKiovh0DABICZ6ugMrKyrRp0ybt2LFD6enpkc91AoGAhg0bpkAgoHvvvVdr1qxRZmamMjIy9MADD6ioqIg74AAAUTwF0IYNGyRJs2fPjtq+ceNGLV++XJL0q1/9SmlpaVq8eLE6OztVUlKiX//613FpFgCQOnzOOWfdxBeFw2EFAgHrNpBExo8f77nmX//6V0xzxXLuvfzyy55rVq1a5bkG6G9CoZAyMjJ63c+z4AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJngaNpLe8ePHPdeMHDkyprmqq6s918yZMyemuYBUx9OwAQBJiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgInB1g0Al/KHP/zBc82PfvSjmObavHlzTHUAvOMKCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAmfc85ZN/FF4XBYgUDAug0AwGUKhULKyMjodT9XQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOEpgCoqKnTzzTcrPT1d2dnZWrBggerq6qKOmT17tnw+X9RYuXJlXJsGAPR/ngKopqZGZWVl2rdvn95++22dO3dOc+fOVXt7e9RxK1asUEtLS2SsW7curk0DAPq/wV4O3rVrV9TryspKZWdn6+DBg5o1a1Zk+5VXXqlgMBifDgEAKemyPgMKhUKSpMzMzKjtr732mrKysjRlyhSVl5frzJkzvX6Pzs5OhcPhqAEAGABcjLq6utz3vvc9N3PmzKjtv/nNb9yuXbvc4cOH3R//+Ed3zTXXuIULF/b6fdauXeskMRgMBiPFRigUumiOxBxAK1eudGPHjnXNzc0XPa6qqspJcvX19T3u7+jocKFQKDKam5vNF43BYDAYlz8uFUCePgP63OrVq7Vz507t3btXo0ePvuixhYWFkqT6+nqNHz/+gv1+v19+vz+WNgAA/ZinAHLO6YEHHtC2bdtUXV2tgoKCS9YcOnRIkpSbmxtTgwCA1OQpgMrKyrRp0ybt2LFD6enpam1tlSQFAgENGzZMDQ0N2rRpk7773e/q6quv1uHDh/Xwww9r1qxZmjp1akL+AACAfsrL5z7q5X2+jRs3Oueca2pqcrNmzXKZmZnO7/e7CRMmuEcfffSS7wN+USgUMn/fksFgMBiXPy71s9/3/4MlaYTDYQUCAes2AACXKRQKKSMjo9f9PAsOAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGAi6QLIOWfdAgAgDi718zzpAujUqVPWLQAA4uBSP899LskuObq7u3X06FGlp6fL5/NF7QuHw8rPz1dzc7MyMjKMOrTHOpzHOpzHOpzHOpyXDOvgnNOpU6eUl5entLTer3MG92FPX0laWppGjx590WMyMjIG9An2OdbhPNbhPNbhPNbhPOt1CAQClzwm6d6CAwAMDAQQAMBEvwogv9+vtWvXyu/3W7diinU4j3U4j3U4j3U4rz+tQ9LdhAAAGBj61RUQACB1EEAAABMEEADABAEEADDRbwJo/fr1uvbaa3XFFVeosLBQ7777rnVLfe7JJ5+Uz+eLGpMnT7ZuK+H27t2rO+64Q3l5efL5fNq+fXvUfuecnnjiCeXm5mrYsGEqLi7WkSNHbJpNoEutw/Llyy84P+bNm2fTbIJUVFTo5ptvVnp6urKzs7VgwQLV1dVFHdPR0aGysjJdffXVGj58uBYvXqy2tjajjhPjq6zD7NmzLzgfVq5cadRxz/pFAG3ZskVr1qzR2rVr9d5772natGkqKSnRsWPHrFvrczfccINaWloi4+9//7t1SwnX3t6uadOmaf369T3uX7dunV544QW99NJL2r9/v6666iqVlJSoo6OjjztNrEutgyTNmzcv6vzYvHlzH3aYeDU1NSorK9O+ffv09ttv69y5c5o7d67a29sjxzz88MN68803tXXrVtXU1Ojo0aNatGiRYdfx91XWQZJWrFgRdT6sW7fOqONeuH5gxowZrqysLPK6q6vL5eXluYqKCsOu+t7atWvdtGnTrNswJclt27Yt8rq7u9sFg0H37LPPRradPHnS+f1+t3nzZoMO+8aX18E555YtW+bmz59v0o+VY8eOOUmupqbGOXf+v/2QIUPc1q1bI8d8+OGHTpKrra21ajPhvrwOzjn3rW99yz344IN2TX0FSX8FdPbsWR08eFDFxcWRbWlpaSouLlZtba1hZzaOHDmivLw8jRs3Tvfcc4+ampqsWzLV2Nio1tbWqPMjEAiosLBwQJ4f1dXVys7O1qRJk7Rq1SqdOHHCuqWECoVCkqTMzExJ0sGDB3Xu3Lmo82Hy5MkaM2ZMSp8PX16Hz7322mvKysrSlClTVF5erjNnzli016ukexjplx0/flxdXV3KycmJ2p6Tk6P//Oc/Rl3ZKCwsVGVlpSZNmqSWlhY99dRTuu222/TBBx8oPT3duj0Tra2tktTj+fH5voFi3rx5WrRokQoKCtTQ0KCf/vSnKi0tVW1trQYNGmTdXtx1d3froYce0syZMzVlyhRJ58+HoUOHasSIEVHHpvL50NM6SNLdd9+tsWPHKi8vT4cPH9ZPfvIT1dXV6c9//rNht9GSPoDwf0pLSyNfT506VYWFhRo7dqzeeOMN3XvvvYadIRksXbo08vWNN96oqVOnavz48aqurtacOXMMO0uMsrIyffDBBwPic9CL6W0d7rvvvsjXN954o3JzczVnzhw1NDRo/Pjxfd1mj5L+LbisrCwNGjTogrtY2traFAwGjbpKDiNGjNDEiRNVX19v3YqZz88Bzo8LjRs3TllZWSl5fqxevVo7d+7Unj17on59SzAY1NmzZ3Xy5Mmo41P1fOhtHXpSWFgoSUl1PiR9AA0dOlTTp09XVVVVZFt3d7eqqqpUVFRk2Jm906dPq6GhQbm5udatmCkoKFAwGIw6P8LhsPbv3z/gz49PPvlEJ06cSKnzwzmn1atXa9u2bdq9e7cKCgqi9k+fPl1DhgyJOh/q6urU1NSUUufDpdahJ4cOHZKk5DofrO+C+Cpef/115/f7XWVlpfv3v//t7rvvPjdixAjX2tpq3Vqf+vGPf+yqq6tdY2Oj+8c//uGKi4tdVlaWO3bsmHVrCXXq1Cn3/vvvu/fff99Jcs8995x7//333ccff+ycc+6Xv/ylGzFihNuxY4c7fPiwmz9/visoKHCfffaZcefxdbF1OHXqlHvkkUdcbW2ta2xsdO+88477xje+4a677jrX0dFh3XrcrFq1ygUCAVddXe1aWloi48yZM5FjVq5c6caMGeN2797tDhw44IqKilxRUZFh1/F3qXWor693P//5z92BAwdcY2Oj27Fjhxs3bpybNWuWcefR+kUAOefciy++6MaMGeOGDh3qZsyY4fbt22fdUp9bsmSJy83NdUOHDnXXXHONW7Jkiauvr7duK+H27NnjJF0wli1b5pw7fyv2448/7nJycpzf73dz5sxxdXV1tk0nwMXW4cyZM27u3Llu1KhRbsiQIW7s2LFuxYoVKfeXtJ7+/JLcxo0bI8d89tln7v7773cjR450V155pVu4cKFraWmxazoBLrUOTU1NbtasWS4zM9P5/X43YcIE9+ijj7pQKGTb+Jfw6xgAACaS/jMgAEBqIoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYOL/AWib0W4BeT6ZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,100,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "09740ee8-10cb-40ad-a315-68999dcbd969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 2369 / 10000\n",
            "Epoch 1 : 4977 / 10000\n",
            "Epoch 2 : 6313 / 10000\n",
            "Epoch 3 : 6523 / 10000\n",
            "Epoch 4 : 7445 / 10000\n",
            "Epoch 5 : 7292 / 10000\n",
            "Epoch 6 : 7757 / 10000\n",
            "Epoch 7 : 7743 / 10000\n",
            "Epoch 8 : 8056 / 10000\n",
            "Epoch 9 : 8131 / 10000\n",
            "Epoch 10 : 8290 / 10000\n",
            "Epoch 11 : 8380 / 10000\n",
            "Epoch 12 : 8410 / 10000\n",
            "Epoch 13 : 8493 / 10000\n",
            "Epoch 14 : 8502 / 10000\n",
            "Epoch 15 : 8572 / 10000\n",
            "Epoch 16 : 8604 / 10000\n",
            "Epoch 17 : 8632 / 10000\n",
            "Epoch 18 : 8672 / 10000\n",
            "Epoch 19 : 8705 / 10000\n",
            "Epoch 20 : 8727 / 10000\n",
            "Epoch 21 : 8770 / 10000\n",
            "Epoch 22 : 8791 / 10000\n",
            "Epoch 23 : 8809 / 10000\n",
            "Epoch 24 : 8825 / 10000\n",
            "Epoch 25 : 8856 / 10000\n",
            "Epoch 26 : 8864 / 10000\n",
            "Epoch 27 : 8888 / 10000\n",
            "Epoch 28 : 8905 / 10000\n",
            "Epoch 29 : 8920 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10000, 3.0, test_data=test_data)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include \n",
        "# 1. the cross-entropy cost function, \n",
        "# 2. regularization, \n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "    \n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "    \n",
        "class CrossEntropyCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    \n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method. \n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "    \n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        \n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "    \n",
        "    def default_weight_initializer(self):\n",
        "        \n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron. \n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "    \n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "            \n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "        \n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    \n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a \n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "        \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "        \n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "            \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "        \n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "                \n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "            \n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "            \n",
        "            print()\n",
        "            \n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "                    \n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "        \n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "    \n",
        "    \n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        \n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "            \n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        \n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    \n",
        "    def accuracy(self, data, convert=False):\n",
        "     \n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        \n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "    \n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "    \n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "        \n",
        "        \n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "    \n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "c3dd6f7b-cd8b-40c4-eb38-ae1e998ebe6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "7d63fb0e-5bc4-4857-8b71-11cb9cb5fdee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3844.1101596254443\n",
            "Accuracy on training data: 46989 / 50000\n",
            "Cost on evaluation data: 3844.100621028285\n",
            "Accuracy on evaluation data: 9428 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4880.695170202227\n",
            "Accuracy on training data: 47541 / 50000\n",
            "Cost on evaluation data: 4880.702145281948\n",
            "Accuracy on evaluation data: 9515 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5411.010117729925\n",
            "Accuracy on training data: 47812 / 50000\n",
            "Cost on evaluation data: 5411.027788550623\n",
            "Accuracy on evaluation data: 9528 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5654.911300665737\n",
            "Accuracy on training data: 47937 / 50000\n",
            "Cost on evaluation data: 5654.9363028559565\n",
            "Accuracy on evaluation data: 9546 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 5842.147690875823\n",
            "Accuracy on training data: 48113 / 50000\n",
            "Cost on evaluation data: 5842.179661866081\n",
            "Accuracy on evaluation data: 9574 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6014.782739047772\n",
            "Accuracy on training data: 47998 / 50000\n",
            "Cost on evaluation data: 6014.818915364679\n",
            "Accuracy on evaluation data: 9547 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6150.517699448858\n",
            "Accuracy on training data: 48114 / 50000\n",
            "Cost on evaluation data: 6150.548112313024\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6191.892394787899\n",
            "Accuracy on training data: 48097 / 50000\n",
            "Cost on evaluation data: 6191.923165550737\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6239.330792495382\n",
            "Accuracy on training data: 48197 / 50000\n",
            "Cost on evaluation data: 6239.362249953372\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6301.366700824118\n",
            "Accuracy on training data: 48279 / 50000\n",
            "Cost on evaluation data: 6301.392972168657\n",
            "Accuracy on evaluation data: 9612 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6338.647726095643\n",
            "Accuracy on training data: 48323 / 50000\n",
            "Cost on evaluation data: 6338.678544220402\n",
            "Accuracy on evaluation data: 9628 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6373.187488435649\n",
            "Accuracy on training data: 48230 / 50000\n",
            "Cost on evaluation data: 6373.232752062508\n",
            "Accuracy on evaluation data: 9569 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6407.559404609965\n",
            "Accuracy on training data: 48265 / 50000\n",
            "Cost on evaluation data: 6407.590935567854\n",
            "Accuracy on evaluation data: 9600 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6389.327214631834\n",
            "Accuracy on training data: 48353 / 50000\n",
            "Cost on evaluation data: 6389.359891199289\n",
            "Accuracy on evaluation data: 9604 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6426.477701215854\n",
            "Accuracy on training data: 48107 / 50000\n",
            "Cost on evaluation data: 6426.504183376902\n",
            "Accuracy on evaluation data: 9588 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6488.00739511136\n",
            "Accuracy on training data: 48073 / 50000\n",
            "Cost on evaluation data: 6488.044942906086\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6502.82923930489\n",
            "Accuracy on training data: 48398 / 50000\n",
            "Cost on evaluation data: 6502.869872549435\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6522.043509337855\n",
            "Accuracy on training data: 48472 / 50000\n",
            "Cost on evaluation data: 6522.075359888333\n",
            "Accuracy on evaluation data: 9659 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6557.575064514933\n",
            "Accuracy on training data: 48390 / 50000\n",
            "Cost on evaluation data: 6557.607994088356\n",
            "Accuracy on evaluation data: 9638 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6525.839110183796\n",
            "Accuracy on training data: 48303 / 50000\n",
            "Cost on evaluation data: 6525.873122129182\n",
            "Accuracy on evaluation data: 9622 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6550.899773627968\n",
            "Accuracy on training data: 48307 / 50000\n",
            "Cost on evaluation data: 6550.940060594596\n",
            "Accuracy on evaluation data: 9609 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6582.739437628389\n",
            "Accuracy on training data: 48433 / 50000\n",
            "Cost on evaluation data: 6582.77906852298\n",
            "Accuracy on evaluation data: 9623 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6564.791561180585\n",
            "Accuracy on training data: 48530 / 50000\n",
            "Cost on evaluation data: 6564.829048962007\n",
            "Accuracy on evaluation data: 9624 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6626.219279963027\n",
            "Accuracy on training data: 48309 / 50000\n",
            "Cost on evaluation data: 6626.255971763323\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6619.6545097743165\n",
            "Accuracy on training data: 48504 / 50000\n",
            "Cost on evaluation data: 6619.693864310016\n",
            "Accuracy on evaluation data: 9638 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6589.551222748201\n",
            "Accuracy on training data: 48115 / 50000\n",
            "Cost on evaluation data: 6589.5762558127135\n",
            "Accuracy on evaluation data: 9584 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6638.587615504992\n",
            "Accuracy on training data: 48529 / 50000\n",
            "Cost on evaluation data: 6638.627087318033\n",
            "Accuracy on evaluation data: 9652 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6632.797140857981\n",
            "Accuracy on training data: 48321 / 50000\n",
            "Cost on evaluation data: 6632.830210377285\n",
            "Accuracy on evaluation data: 9623 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6661.3321967617285\n",
            "Accuracy on training data: 48288 / 50000\n",
            "Cost on evaluation data: 6661.366485846619\n",
            "Accuracy on evaluation data: 9620 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6662.991158692527\n",
            "Accuracy on training data: 48460 / 50000\n",
            "Cost on evaluation data: 6663.0286352833455\n",
            "Accuracy on evaluation data: 9650 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3844.100621028285,\n",
              "  4880.702145281948,\n",
              "  5411.027788550623,\n",
              "  5654.9363028559565,\n",
              "  5842.179661866081,\n",
              "  6014.818915364679,\n",
              "  6150.548112313024,\n",
              "  6191.923165550737,\n",
              "  6239.362249953372,\n",
              "  6301.392972168657,\n",
              "  6338.678544220402,\n",
              "  6373.232752062508,\n",
              "  6407.590935567854,\n",
              "  6389.359891199289,\n",
              "  6426.504183376902,\n",
              "  6488.044942906086,\n",
              "  6502.869872549435,\n",
              "  6522.075359888333,\n",
              "  6557.607994088356,\n",
              "  6525.873122129182,\n",
              "  6550.940060594596,\n",
              "  6582.77906852298,\n",
              "  6564.829048962007,\n",
              "  6626.255971763323,\n",
              "  6619.693864310016,\n",
              "  6589.5762558127135,\n",
              "  6638.627087318033,\n",
              "  6632.830210377285,\n",
              "  6661.366485846619,\n",
              "  6663.0286352833455],\n",
              " [9428,\n",
              "  9515,\n",
              "  9528,\n",
              "  9546,\n",
              "  9574,\n",
              "  9547,\n",
              "  9575,\n",
              "  9566,\n",
              "  9592,\n",
              "  9612,\n",
              "  9628,\n",
              "  9569,\n",
              "  9600,\n",
              "  9604,\n",
              "  9588,\n",
              "  9581,\n",
              "  9615,\n",
              "  9659,\n",
              "  9638,\n",
              "  9622,\n",
              "  9609,\n",
              "  9623,\n",
              "  9624,\n",
              "  9615,\n",
              "  9638,\n",
              "  9584,\n",
              "  9652,\n",
              "  9623,\n",
              "  9620,\n",
              "  9650],\n",
              " [3844.1101596254443,\n",
              "  4880.695170202227,\n",
              "  5411.010117729925,\n",
              "  5654.911300665737,\n",
              "  5842.147690875823,\n",
              "  6014.782739047772,\n",
              "  6150.517699448858,\n",
              "  6191.892394787899,\n",
              "  6239.330792495382,\n",
              "  6301.366700824118,\n",
              "  6338.647726095643,\n",
              "  6373.187488435649,\n",
              "  6407.559404609965,\n",
              "  6389.327214631834,\n",
              "  6426.477701215854,\n",
              "  6488.00739511136,\n",
              "  6502.82923930489,\n",
              "  6522.043509337855,\n",
              "  6557.575064514933,\n",
              "  6525.839110183796,\n",
              "  6550.899773627968,\n",
              "  6582.739437628389,\n",
              "  6564.791561180585,\n",
              "  6626.219279963027,\n",
              "  6619.6545097743165,\n",
              "  6589.551222748201,\n",
              "  6638.587615504992,\n",
              "  6632.797140857981,\n",
              "  6661.3321967617285,\n",
              "  6662.991158692527],\n",
              " [46989,\n",
              "  47541,\n",
              "  47812,\n",
              "  47937,\n",
              "  48113,\n",
              "  47998,\n",
              "  48114,\n",
              "  48097,\n",
              "  48197,\n",
              "  48279,\n",
              "  48323,\n",
              "  48230,\n",
              "  48265,\n",
              "  48353,\n",
              "  48107,\n",
              "  48073,\n",
              "  48398,\n",
              "  48472,\n",
              "  48390,\n",
              "  48303,\n",
              "  48307,\n",
              "  48433,\n",
              "  48530,\n",
              "  48309,\n",
              "  48504,\n",
              "  48115,\n",
              "  48529,\n",
              "  48321,\n",
              "  48288,\n",
              "  48460])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "hIfi3hC8OkON",
        "outputId": "765d279f-c179-4a6b-f1fd-bd56a433f715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}