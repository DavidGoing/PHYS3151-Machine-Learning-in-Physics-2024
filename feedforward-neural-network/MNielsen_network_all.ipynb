{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    \n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "       \n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "            \n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "                \n",
        "    # update the network's weights and biases by applying gradient descent \n",
        "    # using backpropagation to a single mini batch,i.e., for each mini_batch we apply a single step of gradient descent. \n",
        "    # Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) #* \\\n",
        "  #          sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    # the neural network's output is assumed to be the index of \n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "    \n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "    \n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Therefore, to have sufficient update of the weights, it is good to have smaller mini_batch_size**."
      ],
      "metadata": {
        "id": "oWvHo56pn1oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,2,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "8fc49985-383e-4426-df26-d429e219c117"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.64883298],\n",
              "        [ 0.68715592],\n",
              "        [ 0.84538644]]),\n",
              " array([[-0.94010424],\n",
              "        [-0.89888091]]),\n",
              " array([[-1.01602062]])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "38a4915b-d7f0-4e22-f06d-168560f7a609"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.04009933, -1.36815578],\n",
              "        [ 0.34424685,  0.40958709],\n",
              "        [-0.6769856 ,  0.78388171]]),\n",
              " array([[-0.04559171, -0.77248103, -2.50906287],\n",
              "        [ 0.98229804, -0.04901532, -0.60331588]]),\n",
              " array([[-0.46938197, -0.12047035]])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "4876a75a-0eff-4110-88bc-57ba86384ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PHYS3151-Machine-Learning-in-Physics-2023' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "dd4fb741-a2da-4259-b1f7-39d68c46d2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "42ac65aa-1027-41d1-87b1-4c8324adb56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 8944 / 10000\n",
            "Epoch 1 : 9085 / 10000\n",
            "Epoch 2 : 9214 / 10000\n",
            "Epoch 3 : 9235 / 10000\n",
            "Epoch 4 : 9325 / 10000\n",
            "Epoch 5 : 9235 / 10000\n",
            "Epoch 6 : 9305 / 10000\n",
            "Epoch 7 : 9334 / 10000\n",
            "Epoch 8 : 9314 / 10000\n",
            "Epoch 9 : 9381 / 10000\n",
            "Epoch 10 : 9356 / 10000\n",
            "Epoch 11 : 9408 / 10000\n",
            "Epoch 12 : 9410 / 10000\n",
            "Epoch 13 : 9414 / 10000\n",
            "Epoch 14 : 9357 / 10000\n",
            "Epoch 15 : 9470 / 10000\n",
            "Epoch 16 : 9432 / 10000\n",
            "Epoch 17 : 9471 / 10000\n",
            "Epoch 18 : 9459 / 10000\n",
            "Epoch 19 : 9459 / 10000\n",
            "Epoch 20 : 9414 / 10000\n",
            "Epoch 21 : 9443 / 10000\n",
            "Epoch 22 : 9475 / 10000\n",
            "Epoch 23 : 9470 / 10000\n",
            "Epoch 24 : 9339 / 10000\n",
            "Epoch 25 : 9470 / 10000\n",
            "Epoch 26 : 9469 / 10000\n",
            "Epoch 27 : 9469 / 10000\n",
            "Epoch 28 : 9485 / 10000\n",
            "Epoch 29 : 9466 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include \n",
        "# 1. the cross-entropy cost function, \n",
        "# 2. regularization, \n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "    \n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "    \n",
        "class CrossEntropyCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    \n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method. \n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "    \n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        \n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "    \n",
        "    def default_weight_initializer(self):\n",
        "        \n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron. \n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "    \n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "            \n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "        \n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    \n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a \n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "        \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "        \n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "            \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "        \n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "                \n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "            \n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "            \n",
        "            print()\n",
        "            \n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "                    \n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "        \n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "    \n",
        "    \n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        \n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "            \n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        \n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    \n",
        "    def accuracy(self, data, convert=False):\n",
        "     \n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        \n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "    \n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "    \n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "        \n",
        "        \n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "    \n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "9c3c7d98-639e-4214-e756-c1ddb5b33b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "18c90c9e-fe69-4dbd-dcdd-b83904330ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3884.5414394833842\n",
            "Accuracy on training data: 47019 / 50000\n",
            "Cost on evaluation data: 3884.540531377168\n",
            "Accuracy on evaluation data: 9421 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4938.5856859705\n",
            "Accuracy on training data: 47490 / 50000\n",
            "Cost on evaluation data: 4938.6073014432595\n",
            "Accuracy on evaluation data: 9456 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5511.923889302634\n",
            "Accuracy on training data: 47771 / 50000\n",
            "Cost on evaluation data: 5511.941675429273\n",
            "Accuracy on evaluation data: 9536 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5827.614564138921\n",
            "Accuracy on training data: 47998 / 50000\n",
            "Cost on evaluation data: 5827.636434723049\n",
            "Accuracy on evaluation data: 9561 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 5998.959112994284\n",
            "Accuracy on training data: 47944 / 50000\n",
            "Cost on evaluation data: 5998.992548293421\n",
            "Accuracy on evaluation data: 9542 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6120.992095494366\n",
            "Accuracy on training data: 48019 / 50000\n",
            "Cost on evaluation data: 6121.021290007689\n",
            "Accuracy on evaluation data: 9565 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6212.515179853133\n",
            "Accuracy on training data: 48069 / 50000\n",
            "Cost on evaluation data: 6212.5488510464465\n",
            "Accuracy on evaluation data: 9548 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6285.744894095567\n",
            "Accuracy on training data: 48123 / 50000\n",
            "Cost on evaluation data: 6285.7765722451795\n",
            "Accuracy on evaluation data: 9557 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6367.295912957939\n",
            "Accuracy on training data: 48043 / 50000\n",
            "Cost on evaluation data: 6367.335653527608\n",
            "Accuracy on evaluation data: 9528 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6384.309356237339\n",
            "Accuracy on training data: 48060 / 50000\n",
            "Cost on evaluation data: 6384.349945879155\n",
            "Accuracy on evaluation data: 9534 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6432.139082680615\n",
            "Accuracy on training data: 48100 / 50000\n",
            "Cost on evaluation data: 6432.176208839914\n",
            "Accuracy on evaluation data: 9578 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6422.6013478898885\n",
            "Accuracy on training data: 48292 / 50000\n",
            "Cost on evaluation data: 6422.631482570513\n",
            "Accuracy on evaluation data: 9622 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6464.679853944993\n",
            "Accuracy on training data: 48270 / 50000\n",
            "Cost on evaluation data: 6464.716088377129\n",
            "Accuracy on evaluation data: 9597 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6474.822374442882\n",
            "Accuracy on training data: 48273 / 50000\n",
            "Cost on evaluation data: 6474.867704035403\n",
            "Accuracy on evaluation data: 9590 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6488.462355705241\n",
            "Accuracy on training data: 48239 / 50000\n",
            "Cost on evaluation data: 6488.502664766722\n",
            "Accuracy on evaluation data: 9577 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6479.126097180301\n",
            "Accuracy on training data: 48329 / 50000\n",
            "Cost on evaluation data: 6479.171310603353\n",
            "Accuracy on evaluation data: 9590 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6494.157412452423\n",
            "Accuracy on training data: 48334 / 50000\n",
            "Cost on evaluation data: 6494.186839070206\n",
            "Accuracy on evaluation data: 9578 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6515.005487704488\n",
            "Accuracy on training data: 48429 / 50000\n",
            "Cost on evaluation data: 6515.044413146748\n",
            "Accuracy on evaluation data: 9618 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6519.490240838738\n",
            "Accuracy on training data: 48451 / 50000\n",
            "Cost on evaluation data: 6519.533050287879\n",
            "Accuracy on evaluation data: 9608 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6529.87019308786\n",
            "Accuracy on training data: 48245 / 50000\n",
            "Cost on evaluation data: 6529.910083585283\n",
            "Accuracy on evaluation data: 9584 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6564.792603678183\n",
            "Accuracy on training data: 48309 / 50000\n",
            "Cost on evaluation data: 6564.836382935926\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6592.761980008\n",
            "Accuracy on training data: 48177 / 50000\n",
            "Cost on evaluation data: 6592.796693407825\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6580.995442069098\n",
            "Accuracy on training data: 48419 / 50000\n",
            "Cost on evaluation data: 6581.034904013004\n",
            "Accuracy on evaluation data: 9605 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6589.612098938113\n",
            "Accuracy on training data: 48373 / 50000\n",
            "Cost on evaluation data: 6589.660671349106\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6627.902600256147\n",
            "Accuracy on training data: 48473 / 50000\n",
            "Cost on evaluation data: 6627.94661647375\n",
            "Accuracy on evaluation data: 9604 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6657.958083444095\n",
            "Accuracy on training data: 48382 / 50000\n",
            "Cost on evaluation data: 6658.004882263009\n",
            "Accuracy on evaluation data: 9587 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6645.035478858446\n",
            "Accuracy on training data: 48447 / 50000\n",
            "Cost on evaluation data: 6645.079809487293\n",
            "Accuracy on evaluation data: 9614 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6602.612720732028\n",
            "Accuracy on training data: 48046 / 50000\n",
            "Cost on evaluation data: 6602.655213840593\n",
            "Accuracy on evaluation data: 9542 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6629.318526701756\n",
            "Accuracy on training data: 48265 / 50000\n",
            "Cost on evaluation data: 6629.354626339981\n",
            "Accuracy on evaluation data: 9565 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6666.555206868945\n",
            "Accuracy on training data: 48478 / 50000\n",
            "Cost on evaluation data: 6666.60727110258\n",
            "Accuracy on evaluation data: 9601 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3884.540531377168,\n",
              "  4938.6073014432595,\n",
              "  5511.941675429273,\n",
              "  5827.636434723049,\n",
              "  5998.992548293421,\n",
              "  6121.021290007689,\n",
              "  6212.5488510464465,\n",
              "  6285.7765722451795,\n",
              "  6367.335653527608,\n",
              "  6384.349945879155,\n",
              "  6432.176208839914,\n",
              "  6422.631482570513,\n",
              "  6464.716088377129,\n",
              "  6474.867704035403,\n",
              "  6488.502664766722,\n",
              "  6479.171310603353,\n",
              "  6494.186839070206,\n",
              "  6515.044413146748,\n",
              "  6519.533050287879,\n",
              "  6529.910083585283,\n",
              "  6564.836382935926,\n",
              "  6592.796693407825,\n",
              "  6581.034904013004,\n",
              "  6589.660671349106,\n",
              "  6627.94661647375,\n",
              "  6658.004882263009,\n",
              "  6645.079809487293,\n",
              "  6602.655213840593,\n",
              "  6629.354626339981,\n",
              "  6666.60727110258],\n",
              " [9421,\n",
              "  9456,\n",
              "  9536,\n",
              "  9561,\n",
              "  9542,\n",
              "  9565,\n",
              "  9548,\n",
              "  9557,\n",
              "  9528,\n",
              "  9534,\n",
              "  9578,\n",
              "  9622,\n",
              "  9597,\n",
              "  9590,\n",
              "  9577,\n",
              "  9590,\n",
              "  9578,\n",
              "  9618,\n",
              "  9608,\n",
              "  9584,\n",
              "  9575,\n",
              "  9580,\n",
              "  9605,\n",
              "  9592,\n",
              "  9604,\n",
              "  9587,\n",
              "  9614,\n",
              "  9542,\n",
              "  9565,\n",
              "  9601],\n",
              " [3884.5414394833842,\n",
              "  4938.5856859705,\n",
              "  5511.923889302634,\n",
              "  5827.614564138921,\n",
              "  5998.959112994284,\n",
              "  6120.992095494366,\n",
              "  6212.515179853133,\n",
              "  6285.744894095567,\n",
              "  6367.295912957939,\n",
              "  6384.309356237339,\n",
              "  6432.139082680615,\n",
              "  6422.6013478898885,\n",
              "  6464.679853944993,\n",
              "  6474.822374442882,\n",
              "  6488.462355705241,\n",
              "  6479.126097180301,\n",
              "  6494.157412452423,\n",
              "  6515.005487704488,\n",
              "  6519.490240838738,\n",
              "  6529.87019308786,\n",
              "  6564.792603678183,\n",
              "  6592.761980008,\n",
              "  6580.995442069098,\n",
              "  6589.612098938113,\n",
              "  6627.902600256147,\n",
              "  6657.958083444095,\n",
              "  6645.035478858446,\n",
              "  6602.612720732028,\n",
              "  6629.318526701756,\n",
              "  6666.555206868945],\n",
              " [47019,\n",
              "  47490,\n",
              "  47771,\n",
              "  47998,\n",
              "  47944,\n",
              "  48019,\n",
              "  48069,\n",
              "  48123,\n",
              "  48043,\n",
              "  48060,\n",
              "  48100,\n",
              "  48292,\n",
              "  48270,\n",
              "  48273,\n",
              "  48239,\n",
              "  48329,\n",
              "  48334,\n",
              "  48429,\n",
              "  48451,\n",
              "  48245,\n",
              "  48309,\n",
              "  48177,\n",
              "  48419,\n",
              "  48373,\n",
              "  48473,\n",
              "  48382,\n",
              "  48447,\n",
              "  48046,\n",
              "  48265,\n",
              "  48478])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n",
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed",
        "outputId": "20e70532-9374-47ca-8c75-ca7e198860f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 979.9962913547373\n",
            "Accuracy on training data: 45858 / 50000\n",
            "Cost on evaluation data: 979.9895181695631\n",
            "Accuracy on evaluation data: 9265 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 1192.9120701184258\n",
            "Accuracy on training data: 46502 / 50000\n",
            "Cost on evaluation data: 1192.9076332756356\n",
            "Accuracy on evaluation data: 9345 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 1301.1379549852113\n",
            "Accuracy on training data: 46969 / 50000\n",
            "Cost on evaluation data: 1301.1348307278129\n",
            "Accuracy on evaluation data: 9424 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 1365.9768534882196\n",
            "Accuracy on training data: 47177 / 50000\n",
            "Cost on evaluation data: 1365.9737935668495\n",
            "Accuracy on evaluation data: 9473 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 1410.3320926285162\n",
            "Accuracy on training data: 47226 / 50000\n",
            "Cost on evaluation data: 1410.3295347067851\n",
            "Accuracy on evaluation data: 9479 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 1435.4591481086177\n",
            "Accuracy on training data: 47348 / 50000\n",
            "Cost on evaluation data: 1435.4573337824231\n",
            "Accuracy on evaluation data: 9504 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 1454.4613729986822\n",
            "Accuracy on training data: 47414 / 50000\n",
            "Cost on evaluation data: 1454.45933973366\n",
            "Accuracy on evaluation data: 9523 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 1466.8759981174594\n",
            "Accuracy on training data: 47502 / 50000\n",
            "Cost on evaluation data: 1466.8744028862056\n",
            "Accuracy on evaluation data: 9540 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 1479.7028279455815\n",
            "Accuracy on training data: 47622 / 50000\n",
            "Cost on evaluation data: 1479.7012418141137\n",
            "Accuracy on evaluation data: 9565 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 1485.1381972320264\n",
            "Accuracy on training data: 47662 / 50000\n",
            "Cost on evaluation data: 1485.1364894322433\n",
            "Accuracy on evaluation data: 9568 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 1492.6150020238715\n",
            "Accuracy on training data: 47670 / 50000\n",
            "Cost on evaluation data: 1492.6134791703694\n",
            "Accuracy on evaluation data: 9580 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 1493.1199242510186\n",
            "Accuracy on training data: 47591 / 50000\n",
            "Cost on evaluation data: 1493.1184890568684\n",
            "Accuracy on evaluation data: 9557 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 1497.2297459191204\n",
            "Accuracy on training data: 47610 / 50000\n",
            "Cost on evaluation data: 1497.2280627510754\n",
            "Accuracy on evaluation data: 9562 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 1500.4112725155542\n",
            "Accuracy on training data: 47677 / 50000\n",
            "Cost on evaluation data: 1500.4098687370072\n",
            "Accuracy on evaluation data: 9559 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 1502.3382484743654\n",
            "Accuracy on training data: 47618 / 50000\n",
            "Cost on evaluation data: 1502.3370051615125\n",
            "Accuracy on evaluation data: 9551 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 1503.8797399818984\n",
            "Accuracy on training data: 47667 / 50000\n",
            "Cost on evaluation data: 1503.8785652926401\n",
            "Accuracy on evaluation data: 9558 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 1506.4502542353266\n",
            "Accuracy on training data: 47707 / 50000\n",
            "Cost on evaluation data: 1506.4488876230419\n",
            "Accuracy on evaluation data: 9573 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 1508.1896005324595\n",
            "Accuracy on training data: 47729 / 50000\n",
            "Cost on evaluation data: 1508.1888634015088\n",
            "Accuracy on evaluation data: 9571 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 1508.3628426413695\n",
            "Accuracy on training data: 47788 / 50000\n",
            "Cost on evaluation data: 1508.3613128277566\n",
            "Accuracy on evaluation data: 9590 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 1508.4819499504622\n",
            "Accuracy on training data: 47817 / 50000\n",
            "Cost on evaluation data: 1508.4805186321892\n",
            "Accuracy on evaluation data: 9599 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 1508.723072842164\n",
            "Accuracy on training data: 47681 / 50000\n",
            "Cost on evaluation data: 1508.721670745115\n",
            "Accuracy on evaluation data: 9568 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 1509.264202262471\n",
            "Accuracy on training data: 47767 / 50000\n",
            "Cost on evaluation data: 1509.2632137503992\n",
            "Accuracy on evaluation data: 9571 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 1510.410661583034\n",
            "Accuracy on training data: 47751 / 50000\n",
            "Cost on evaluation data: 1510.409350947892\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 1513.7452358796272\n",
            "Accuracy on training data: 47827 / 50000\n",
            "Cost on evaluation data: 1513.7437063154014\n",
            "Accuracy on evaluation data: 9589 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 1514.0273329877243\n",
            "Accuracy on training data: 47719 / 50000\n",
            "Cost on evaluation data: 1514.0262147113892\n",
            "Accuracy on evaluation data: 9560 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 1511.2241294422076\n",
            "Accuracy on training data: 47774 / 50000\n",
            "Cost on evaluation data: 1511.2230031369995\n",
            "Accuracy on evaluation data: 9574 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 1512.5471977425257\n",
            "Accuracy on training data: 47827 / 50000\n",
            "Cost on evaluation data: 1512.546077802647\n",
            "Accuracy on evaluation data: 9576 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 1514.1186620033031\n",
            "Accuracy on training data: 47866 / 50000\n",
            "Cost on evaluation data: 1514.1176869296175\n",
            "Accuracy on evaluation data: 9593 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 1516.1651619424597\n",
            "Accuracy on training data: 47819 / 50000\n",
            "Cost on evaluation data: 1516.1636780291653\n",
            "Accuracy on evaluation data: 9573 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 1513.6418782419728\n",
            "Accuracy on training data: 47864 / 50000\n",
            "Cost on evaluation data: 1513.6403264018998\n",
            "Accuracy on evaluation data: 9599 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([979.9895181695631,\n",
              "  1192.9076332756356,\n",
              "  1301.1348307278129,\n",
              "  1365.9737935668495,\n",
              "  1410.3295347067851,\n",
              "  1435.4573337824231,\n",
              "  1454.45933973366,\n",
              "  1466.8744028862056,\n",
              "  1479.7012418141137,\n",
              "  1485.1364894322433,\n",
              "  1492.6134791703694,\n",
              "  1493.1184890568684,\n",
              "  1497.2280627510754,\n",
              "  1500.4098687370072,\n",
              "  1502.3370051615125,\n",
              "  1503.8785652926401,\n",
              "  1506.4488876230419,\n",
              "  1508.1888634015088,\n",
              "  1508.3613128277566,\n",
              "  1508.4805186321892,\n",
              "  1508.721670745115,\n",
              "  1509.2632137503992,\n",
              "  1510.409350947892,\n",
              "  1513.7437063154014,\n",
              "  1514.0262147113892,\n",
              "  1511.2230031369995,\n",
              "  1512.546077802647,\n",
              "  1514.1176869296175,\n",
              "  1516.1636780291653,\n",
              "  1513.6403264018998],\n",
              " [9265,\n",
              "  9345,\n",
              "  9424,\n",
              "  9473,\n",
              "  9479,\n",
              "  9504,\n",
              "  9523,\n",
              "  9540,\n",
              "  9565,\n",
              "  9568,\n",
              "  9580,\n",
              "  9557,\n",
              "  9562,\n",
              "  9559,\n",
              "  9551,\n",
              "  9558,\n",
              "  9573,\n",
              "  9571,\n",
              "  9590,\n",
              "  9599,\n",
              "  9568,\n",
              "  9571,\n",
              "  9575,\n",
              "  9589,\n",
              "  9560,\n",
              "  9574,\n",
              "  9576,\n",
              "  9593,\n",
              "  9573,\n",
              "  9599],\n",
              " [979.9962913547373,\n",
              "  1192.9120701184258,\n",
              "  1301.1379549852113,\n",
              "  1365.9768534882196,\n",
              "  1410.3320926285162,\n",
              "  1435.4591481086177,\n",
              "  1454.4613729986822,\n",
              "  1466.8759981174594,\n",
              "  1479.7028279455815,\n",
              "  1485.1381972320264,\n",
              "  1492.6150020238715,\n",
              "  1493.1199242510186,\n",
              "  1497.2297459191204,\n",
              "  1500.4112725155542,\n",
              "  1502.3382484743654,\n",
              "  1503.8797399818984,\n",
              "  1506.4502542353266,\n",
              "  1508.1896005324595,\n",
              "  1508.3628426413695,\n",
              "  1508.4819499504622,\n",
              "  1508.723072842164,\n",
              "  1509.264202262471,\n",
              "  1510.410661583034,\n",
              "  1513.7452358796272,\n",
              "  1514.0273329877243,\n",
              "  1511.2241294422076,\n",
              "  1512.5471977425257,\n",
              "  1514.1186620033031,\n",
              "  1516.1651619424597,\n",
              "  1513.6418782419728],\n",
              " [45858,\n",
              "  46502,\n",
              "  46969,\n",
              "  47177,\n",
              "  47226,\n",
              "  47348,\n",
              "  47414,\n",
              "  47502,\n",
              "  47622,\n",
              "  47662,\n",
              "  47670,\n",
              "  47591,\n",
              "  47610,\n",
              "  47677,\n",
              "  47618,\n",
              "  47667,\n",
              "  47707,\n",
              "  47729,\n",
              "  47788,\n",
              "  47817,\n",
              "  47681,\n",
              "  47767,\n",
              "  47751,\n",
              "  47827,\n",
              "  47719,\n",
              "  47774,\n",
              "  47827,\n",
              "  47866,\n",
              "  47819,\n",
              "  47864])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}