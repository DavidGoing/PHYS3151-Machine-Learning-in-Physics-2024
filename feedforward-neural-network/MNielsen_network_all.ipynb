{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/feedforward-neural-network/MNielsen_network_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dXoM7ZwcSdGv"
      },
      "outputs": [],
      "source": [
        "# A module to implement the gradient descent learning algorithm for a feedforward neural network.\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Network(object):\n",
        "    \n",
        "    # the list ''sizes'' contains the number of neurons in the respective layers of the network.\n",
        "    # [2, 3, 1] input layer 2 neurons, hidden layer 3 neurons, output layer 1 neuron\n",
        "    def __init__(self, sizes):\n",
        "\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "       \n",
        "    # return the output of the network if \"a\" is input.\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    # “training_data” is a list of tuples \"(x,y)\" representing the training inputs and the desired outputs.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if test_data:\n",
        "            test_data = list(test_data)\n",
        "            n_test = len(test_data)\n",
        "            \n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
        "            else:\n",
        "                print(\"Epoch {} complete\".format(j))\n",
        "                \n",
        "    # update the network's weights and biases by applying gradient descent \n",
        "    # using backpropagation to a single mini batch.\n",
        "    # The ''mini_batch'' is a list of tuples \"(x,y)\", and \"eta\" is the learning rate.\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                       for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) #* \\\n",
        "  #          sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # l=1 means the last layer of the neurons\n",
        "        # l=2 means the second-last layer\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    # the neural network's output is assumed to be the index of \n",
        "    # whichever neuron in the final layer has the highest activation.\n",
        "    def evaluate(self, test_data):\n",
        "        test_results = [(np.argmax(self.feedforward(x)),y)\n",
        "                       for (x,y) in test_data]\n",
        "        return sum(int(x == y) for (x,y) in test_results)\n",
        "    \n",
        "    # vector of partial derivatives \\partial C_X / \\partial a for the output activations.\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "    \n",
        "# Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neural_network = Network([2,3,1])\n",
        "# show the random biases\n",
        "neural_network.biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7xJiLiBUSl",
        "outputId": "3c8bf47d-a682-4799-cf31-f6cfc4840f20"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.2012548 ],\n",
              "        [ 0.34268377],\n",
              "        [-1.05331073]]),\n",
              " array([[0.05745517]])]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show the random weights\n",
        "neural_network.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzcbJs2CMQ-",
        "outputId": "e0c4e47b-6c01-4a83-b3c4-86f15fc70360"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.80252056, -0.25635687],\n",
              "        [ 0.84055294,  0.05929157],\n",
              "        [-1.12097732, -1.38434122]]),\n",
              " array([[0.1591467 , 0.73770062, 0.50911051]])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023"
      ],
      "metadata": {
        "id": "RoEQB1tkTO4i",
        "outputId": "43cfd3bb-cb57-4bc6-97f7-a8fb29922c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PHYS3151-Machine-Learning-in-Physics-2023'...\n",
            "remote: Enumerating objects: 588, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 588 (delta 95), reused 114 (delta 62), pack-reused 417\u001b[K\n",
            "Receiving objects: 100% (588/588), 30.43 MiB | 21.88 MiB/s, done.\n",
            "Resolving deltas: 100% (312/312), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9bt3SXZUSdGy"
      },
      "outputs": [],
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[4999].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a 10-dimensional unit vector with a 1.0 in the jth position and zeros elsewhere.\n",
        "# This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5k-jTJDrSdGy",
        "outputId": "ce3dc394-edfd-4a5b-8157-905d6288d7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iqQPTo_mSdGz"
      },
      "outputs": [],
      "source": [
        "net = Network([784,30,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "k2frayMZSdGz",
        "outputId": "50576ec7-a7ec-43e1-cf6b-119bae71085d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 1912 / 10000\n",
            "Epoch 1 : 3837 / 10000\n",
            "Epoch 2 : 4894 / 10000\n",
            "Epoch 3 : 5674 / 10000\n",
            "Epoch 4 : 6189 / 10000\n",
            "Epoch 5 : 6525 / 10000\n",
            "Epoch 6 : 6813 / 10000\n",
            "Epoch 7 : 7029 / 10000\n",
            "Epoch 8 : 7196 / 10000\n",
            "Epoch 9 : 7353 / 10000\n",
            "Epoch 10 : 7482 / 10000\n",
            "Epoch 11 : 7588 / 10000\n",
            "Epoch 12 : 7710 / 10000\n",
            "Epoch 13 : 7789 / 10000\n",
            "Epoch 14 : 7851 / 10000\n",
            "Epoch 15 : 7924 / 10000\n",
            "Epoch 16 : 8001 / 10000\n",
            "Epoch 17 : 8054 / 10000\n",
            "Epoch 18 : 8105 / 10000\n",
            "Epoch 19 : 8140 / 10000\n",
            "Epoch 20 : 8181 / 10000\n",
            "Epoch 21 : 8211 / 10000\n",
            "Epoch 22 : 8243 / 10000\n",
            "Epoch 23 : 8279 / 10000\n",
            "Epoch 24 : 8312 / 10000\n",
            "Epoch 25 : 8335 / 10000\n",
            "Epoch 26 : 8367 / 10000\n",
            "Epoch 27 : 8395 / 10000\n",
            "Epoch 28 : 8415 / 10000\n",
            "Epoch 29 : 8449 / 10000\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10000, 3.0, test_data=test_data)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRs0ll3ISdGz"
      },
      "outputs": [],
      "source": [
        "# An improved version of MNielsen_network, implementing the SGD learning algorithm for a feedforward neural network.\n",
        "# Improvments include \n",
        "# 1. the cross-entropy cost function, \n",
        "# 2. regularization, \n",
        "# 3. better initialization of the weights.\n",
        "\n",
        "import json\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "# define the quadratic and cross-entropy cost functions\n",
        "\n",
        "class QuadraticCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "    \n",
        "    # return the error delta from the output layer.\n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)*sigmoid_prime(z)\n",
        "    \n",
        "class CrossEntropyCost(object):\n",
        "    \n",
        "    # return the cost associated with an output \"a\" and desired output \"y\".\n",
        "    # np.nan_to_num is used to ensure numerical stability, make sure 0*log(0) = 0.0\n",
        "    @staticmethod\n",
        "    def fn(a, y):\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    \n",
        "    # returm the error delta from the output layer.\n",
        "    # parameter \"z\" is not used by the method. \n",
        "    @staticmethod\n",
        "    def delta(z, a, y):\n",
        "        return (a-y)\n",
        "    \n",
        "# Main Network class\n",
        "class Network2(object):\n",
        "    \n",
        "    \n",
        "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
        "        \n",
        "        # the biases and weights for the network are initiated randomly, using \"self.default_weight_initializer\".\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.default_weight_initializer()\n",
        "        self.cost = cost\n",
        "    \n",
        "    def default_weight_initializer(self):\n",
        "        \n",
        "        # initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1\n",
        "        # over the square root of the number of weights connecting to the same neuron. \n",
        "        # initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1.\n",
        "    \n",
        "        # for the input layers, there is no biases.\n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "\n",
        "    def large_weight_initializer(self):\n",
        "            \n",
        "        self.biases = [np.random.randn(y,1) for y in self.sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
        "        \n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "    \n",
        "    # \"evaluation_data\", monitor the cost and accuracy on either the evaluation data or the training data.\n",
        "    # returns a tuple containing four lists:\n",
        "    # 1. the (per-epoch) costs on the evaluation data,\n",
        "    # 2. the accuracies on the evaluation data,\n",
        "    # 3. the costs on the training data,\n",
        "    # 4. the accuracies on the training data.\n",
        "    # If we train for 30 epochs, then the first element of the tuple will be a \n",
        "    # 30-element list containing the cost on the evaluation data at the end of each epoch.\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0,\n",
        "            evaluation_data = None,\n",
        "            monitor_evaluation_cost = False,\n",
        "            monitor_evaluation_accuracy = False,\n",
        "            monitor_training_cost = False,\n",
        "            monitor_training_accuracy = False,\n",
        "            early_stopping_n = 0):\n",
        "        \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=1\n",
        "        \n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        \n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "            \n",
        "        # early stopping functionality:\n",
        "        best_accuracy=0\n",
        "        no_accuracy_change=0\n",
        "        \n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "                \n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "            \n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "            \n",
        "            print()\n",
        "            \n",
        "            # Early stopping:\n",
        "            if early_stopping_n > 0:\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    no_accuracy_change = 0\n",
        "                    print(\"Early-stopping: Best so far{}\".format(best_accuracy))\n",
        "                else:\n",
        "                    no_accuracy_change += 1\n",
        "                    \n",
        "                if (no_accuracy_change == early_stopping_n):\n",
        "                    print(\"Early-stopping: No accuracy cahnge in last epochs: {}\".format(early_stopping_n))\n",
        "                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "        \n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "    \n",
        "    \n",
        "    # \"mini_batch\" is a list of tuples \"(x,y)\"\n",
        "    # \"eta\" is the learning rate\n",
        "    # \"lmbda\" is the regularization parameter\n",
        "    # \"n\" is the total size of the training set.\n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
        "        \n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        # \"eta*(lmbda/n)*w\" comes from the regularization term.\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    # return a tuple \"(nabla_b, nabla_w)\" representing the gradient for the cost function.\n",
        "    # \"nabla_b\" and \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "    def backprop(self, x, y):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "            \n",
        "        # backpropagate\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        \n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "    \n",
        "    \n",
        "    def accuracy(self, data, convert=False):\n",
        "     \n",
        "        if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "        else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "        \n",
        "        result_accuracy = sum(int(x==y) for (x,y) in results)\n",
        "        return result_accuracy\n",
        "    \n",
        "    def total_cost(self, data, lmbda, convert=False):\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "            cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "        return cost\n",
        "    \n",
        "    # Save the neural network to the file \"filename\".\n",
        "    def save(self, filename):\n",
        "        data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "        f = open(filename, \"w\")\n",
        "        json.dump(data, f)\n",
        "        f.close()\n",
        "        \n",
        "        \n",
        "# Loading a Network\n",
        "def load(filename):\n",
        "    \n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "\n",
        "# Miscellaneous functions\n",
        "def vectorized_result(j):\n",
        "    \n",
        "    e = np.zeros((10,1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A library to load the MNIST image data.\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Return the MNIST data as a tuple containing the training data, the validation data, and the test data.\n",
        "# The \"training_data\" is returned as a tuple with two entries.\n",
        "\n",
        "# The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, \n",
        "# in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image.\n",
        "\n",
        "# The second entry in the \"training_data\" tuple is a numpy ndarray containing 50,000 entries. Those entries\n",
        "# are just the digit values (0,1,...,9) for the corresponding images contained in the first entry of the tuple.\n",
        "\n",
        "# The \"validation_data\" and \"test_data\" are similar, except each contains only 10,000 images.\n",
        "def load_data():\n",
        "\n",
        "        f = gzip.open('/content/PHYS3151-Machine-Learning-in-Physics-2023/feedforward-neural-network/mnist.pkl.gz','rb')\n",
        "        training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "        f.close()\n",
        "        return (training_data, validation_data, test_data)\n",
        "\n",
        "# Return a tuple containing \"(training_data, validation_data, test_data)\".\n",
        "# \"training_data\" is a list containing 50,000 2-tuples \"(x,y)\". \n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image.\n",
        "# \"y\" is a 10-dimensional numpy.ndarray representing the unit vector \n",
        "# corresponding to the correct digit for \"x\".\n",
        "# \"validation_data\" and \"test_data\" are lists containing 10,000 2-tuples \"(x,y)\". In each case,\n",
        "# \"x\" is a 784-dimensional numpy.ndarray containing the input image, and\n",
        "# \"y\" is the corresponding classification, i.e., the digit values (integers) corresponding to \"x\".\n",
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784,1)) for x in tr_d[0]]\n",
        "    \n",
        "    plt.imshow(training_inputs[1].reshape((28,28)),cmap=cm.Greys_r)\n",
        "    plt.show()\n",
        "    \n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784,1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784,1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "metadata": {
        "id": "4KCSryRc-XA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "czAce4vR-Y_-",
        "outputId": "c3dd6f7b-cd8b-40c4-eb38-ae1e998ebe6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net2 = Network2([784, 30, 10], cost=CrossEntropyCost)\n",
        "net2.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "m3vhgcz_-bEl",
        "outputId": "7d63fb0e-5bc4-4857-8b71-11cb9cb5fdee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 3844.1101596254443\n",
            "Accuracy on training data: 46989 / 50000\n",
            "Cost on evaluation data: 3844.100621028285\n",
            "Accuracy on evaluation data: 9428 / 10000\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 4880.695170202227\n",
            "Accuracy on training data: 47541 / 50000\n",
            "Cost on evaluation data: 4880.702145281948\n",
            "Accuracy on evaluation data: 9515 / 10000\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 5411.010117729925\n",
            "Accuracy on training data: 47812 / 50000\n",
            "Cost on evaluation data: 5411.027788550623\n",
            "Accuracy on evaluation data: 9528 / 10000\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 5654.911300665737\n",
            "Accuracy on training data: 47937 / 50000\n",
            "Cost on evaluation data: 5654.9363028559565\n",
            "Accuracy on evaluation data: 9546 / 10000\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 5842.147690875823\n",
            "Accuracy on training data: 48113 / 50000\n",
            "Cost on evaluation data: 5842.179661866081\n",
            "Accuracy on evaluation data: 9574 / 10000\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 6014.782739047772\n",
            "Accuracy on training data: 47998 / 50000\n",
            "Cost on evaluation data: 6014.818915364679\n",
            "Accuracy on evaluation data: 9547 / 10000\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 6150.517699448858\n",
            "Accuracy on training data: 48114 / 50000\n",
            "Cost on evaluation data: 6150.548112313024\n",
            "Accuracy on evaluation data: 9575 / 10000\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 6191.892394787899\n",
            "Accuracy on training data: 48097 / 50000\n",
            "Cost on evaluation data: 6191.923165550737\n",
            "Accuracy on evaluation data: 9566 / 10000\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 6239.330792495382\n",
            "Accuracy on training data: 48197 / 50000\n",
            "Cost on evaluation data: 6239.362249953372\n",
            "Accuracy on evaluation data: 9592 / 10000\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 6301.366700824118\n",
            "Accuracy on training data: 48279 / 50000\n",
            "Cost on evaluation data: 6301.392972168657\n",
            "Accuracy on evaluation data: 9612 / 10000\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 6338.647726095643\n",
            "Accuracy on training data: 48323 / 50000\n",
            "Cost on evaluation data: 6338.678544220402\n",
            "Accuracy on evaluation data: 9628 / 10000\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 6373.187488435649\n",
            "Accuracy on training data: 48230 / 50000\n",
            "Cost on evaluation data: 6373.232752062508\n",
            "Accuracy on evaluation data: 9569 / 10000\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 6407.559404609965\n",
            "Accuracy on training data: 48265 / 50000\n",
            "Cost on evaluation data: 6407.590935567854\n",
            "Accuracy on evaluation data: 9600 / 10000\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 6389.327214631834\n",
            "Accuracy on training data: 48353 / 50000\n",
            "Cost on evaluation data: 6389.359891199289\n",
            "Accuracy on evaluation data: 9604 / 10000\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 6426.477701215854\n",
            "Accuracy on training data: 48107 / 50000\n",
            "Cost on evaluation data: 6426.504183376902\n",
            "Accuracy on evaluation data: 9588 / 10000\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 6488.00739511136\n",
            "Accuracy on training data: 48073 / 50000\n",
            "Cost on evaluation data: 6488.044942906086\n",
            "Accuracy on evaluation data: 9581 / 10000\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 6502.82923930489\n",
            "Accuracy on training data: 48398 / 50000\n",
            "Cost on evaluation data: 6502.869872549435\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 6522.043509337855\n",
            "Accuracy on training data: 48472 / 50000\n",
            "Cost on evaluation data: 6522.075359888333\n",
            "Accuracy on evaluation data: 9659 / 10000\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 6557.575064514933\n",
            "Accuracy on training data: 48390 / 50000\n",
            "Cost on evaluation data: 6557.607994088356\n",
            "Accuracy on evaluation data: 9638 / 10000\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 6525.839110183796\n",
            "Accuracy on training data: 48303 / 50000\n",
            "Cost on evaluation data: 6525.873122129182\n",
            "Accuracy on evaluation data: 9622 / 10000\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 6550.899773627968\n",
            "Accuracy on training data: 48307 / 50000\n",
            "Cost on evaluation data: 6550.940060594596\n",
            "Accuracy on evaluation data: 9609 / 10000\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 6582.739437628389\n",
            "Accuracy on training data: 48433 / 50000\n",
            "Cost on evaluation data: 6582.77906852298\n",
            "Accuracy on evaluation data: 9623 / 10000\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 6564.791561180585\n",
            "Accuracy on training data: 48530 / 50000\n",
            "Cost on evaluation data: 6564.829048962007\n",
            "Accuracy on evaluation data: 9624 / 10000\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 6626.219279963027\n",
            "Accuracy on training data: 48309 / 50000\n",
            "Cost on evaluation data: 6626.255971763323\n",
            "Accuracy on evaluation data: 9615 / 10000\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 6619.6545097743165\n",
            "Accuracy on training data: 48504 / 50000\n",
            "Cost on evaluation data: 6619.693864310016\n",
            "Accuracy on evaluation data: 9638 / 10000\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 6589.551222748201\n",
            "Accuracy on training data: 48115 / 50000\n",
            "Cost on evaluation data: 6589.5762558127135\n",
            "Accuracy on evaluation data: 9584 / 10000\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 6638.587615504992\n",
            "Accuracy on training data: 48529 / 50000\n",
            "Cost on evaluation data: 6638.627087318033\n",
            "Accuracy on evaluation data: 9652 / 10000\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 6632.797140857981\n",
            "Accuracy on training data: 48321 / 50000\n",
            "Cost on evaluation data: 6632.830210377285\n",
            "Accuracy on evaluation data: 9623 / 10000\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 6661.3321967617285\n",
            "Accuracy on training data: 48288 / 50000\n",
            "Cost on evaluation data: 6661.366485846619\n",
            "Accuracy on evaluation data: 9620 / 10000\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 6662.991158692527\n",
            "Accuracy on training data: 48460 / 50000\n",
            "Cost on evaluation data: 6663.0286352833455\n",
            "Accuracy on evaluation data: 9650 / 10000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3844.100621028285,\n",
              "  4880.702145281948,\n",
              "  5411.027788550623,\n",
              "  5654.9363028559565,\n",
              "  5842.179661866081,\n",
              "  6014.818915364679,\n",
              "  6150.548112313024,\n",
              "  6191.923165550737,\n",
              "  6239.362249953372,\n",
              "  6301.392972168657,\n",
              "  6338.678544220402,\n",
              "  6373.232752062508,\n",
              "  6407.590935567854,\n",
              "  6389.359891199289,\n",
              "  6426.504183376902,\n",
              "  6488.044942906086,\n",
              "  6502.869872549435,\n",
              "  6522.075359888333,\n",
              "  6557.607994088356,\n",
              "  6525.873122129182,\n",
              "  6550.940060594596,\n",
              "  6582.77906852298,\n",
              "  6564.829048962007,\n",
              "  6626.255971763323,\n",
              "  6619.693864310016,\n",
              "  6589.5762558127135,\n",
              "  6638.627087318033,\n",
              "  6632.830210377285,\n",
              "  6661.366485846619,\n",
              "  6663.0286352833455],\n",
              " [9428,\n",
              "  9515,\n",
              "  9528,\n",
              "  9546,\n",
              "  9574,\n",
              "  9547,\n",
              "  9575,\n",
              "  9566,\n",
              "  9592,\n",
              "  9612,\n",
              "  9628,\n",
              "  9569,\n",
              "  9600,\n",
              "  9604,\n",
              "  9588,\n",
              "  9581,\n",
              "  9615,\n",
              "  9659,\n",
              "  9638,\n",
              "  9622,\n",
              "  9609,\n",
              "  9623,\n",
              "  9624,\n",
              "  9615,\n",
              "  9638,\n",
              "  9584,\n",
              "  9652,\n",
              "  9623,\n",
              "  9620,\n",
              "  9650],\n",
              " [3844.1101596254443,\n",
              "  4880.695170202227,\n",
              "  5411.010117729925,\n",
              "  5654.911300665737,\n",
              "  5842.147690875823,\n",
              "  6014.782739047772,\n",
              "  6150.517699448858,\n",
              "  6191.892394787899,\n",
              "  6239.330792495382,\n",
              "  6301.366700824118,\n",
              "  6338.647726095643,\n",
              "  6373.187488435649,\n",
              "  6407.559404609965,\n",
              "  6389.327214631834,\n",
              "  6426.477701215854,\n",
              "  6488.00739511136,\n",
              "  6502.82923930489,\n",
              "  6522.043509337855,\n",
              "  6557.575064514933,\n",
              "  6525.839110183796,\n",
              "  6550.899773627968,\n",
              "  6582.739437628389,\n",
              "  6564.791561180585,\n",
              "  6626.219279963027,\n",
              "  6619.6545097743165,\n",
              "  6589.551222748201,\n",
              "  6638.587615504992,\n",
              "  6632.797140857981,\n",
              "  6661.3321967617285,\n",
              "  6662.991158692527],\n",
              " [46989,\n",
              "  47541,\n",
              "  47812,\n",
              "  47937,\n",
              "  48113,\n",
              "  47998,\n",
              "  48114,\n",
              "  48097,\n",
              "  48197,\n",
              "  48279,\n",
              "  48323,\n",
              "  48230,\n",
              "  48265,\n",
              "  48353,\n",
              "  48107,\n",
              "  48073,\n",
              "  48398,\n",
              "  48472,\n",
              "  48390,\n",
              "  48303,\n",
              "  48307,\n",
              "  48433,\n",
              "  48530,\n",
              "  48309,\n",
              "  48504,\n",
              "  48115,\n",
              "  48529,\n",
              "  48321,\n",
              "  48288,\n",
              "  48460])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic = Network2([784, 30, 10], cost=QuadraticCost)\n"
      ],
      "metadata": {
        "id": "JlPtN5Wa-fed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netQuadratic.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,monitor_evaluation_accuracy=True,\n",
        "        monitor_evaluation_cost=True,monitor_training_accuracy=True,monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "hIfi3hC8OkON",
        "outputId": "765d279f-c179-4a6b-f1fd-bd56a433f715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 1 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 2 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 3 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 4 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 5 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 6 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 7 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 8 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 9 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 10 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 11 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 12 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 13 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 14 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 15 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 16 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 17 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 18 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 19 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 20 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 21 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 22 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 23 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 24 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 25 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 26 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 27 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 28 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n",
            "Epoch 29 training complete\n",
            "Cost on training data: 0.0\n",
            "Accuracy on training data: 0 / 0\n",
            "Cost on evaluation data: 0.0\n",
            "Accuracy on evaluation data: 0 / 0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "MNielsen_network.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}