{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzXvL/3KgKsgfctw/9v6dX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoisWTT/PHYS3151-Machine-Learning-in-Physics-2023/blob/main/logistic-regression/Grading_weight.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Layer"
      ],
      "metadata": {
        "id": "cDU-rIlaZqzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the grading method of this course.  Four assignments account for in total 30%, a project and a presentation each takes up 20%, and a final exam accounts for 30%.\n",
        "Consider a (fictitious) grading cut: F(<50), D(50\\~60), C(60\\~75), B(75\\~90) A(90\\~100), without any subdivision. Assign an integer from 0 to 4 to each of them, with 0 being F and 4 being A."
      ],
      "metadata": {
        "id": "PPMUgOX2ZqzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import csv"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "HbAYw2YxZqzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "letter_grade=[\"F\",\"D\",\"C\",\"B\",\"A\"]\n",
        "weight=[0.075, 0.075, 0.075, 0.075, 0.2, 0.2, 0.3]\n",
        "np.random.seed\n",
        "with open('grade_data.csv', mode='w') as sample_file:\n",
        "    grade_writer = csv.writer(sample_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    grade_writer.writerow(['HW1','HW2','HW3','HW4','Project','Presentation','Exam','Grade',])\n",
        "    for i in range (0, 1000):\n",
        "        mark_temp=np.random.normal(60, 60, 7)\n",
        "        #prevent marks from going out of the range [0,100]\n",
        "        mark_temp=[max(p,0) for p in mark_temp]\n",
        "        mark_temp=[min(p,100) for p in mark_temp]\n",
        "        mark_weighted=np.dot(weight,mark_temp)\n",
        "        grade=0\n",
        "        if mark_weighted>=50:\n",
        "          grade+=1\n",
        "          if mark_weighted>=60:\n",
        "            grade+=1\n",
        "            if mark_weighted>=75:\n",
        "              grade+=1\n",
        "              if mark_weighted>=90:\n",
        "                grade+=1\n",
        "        mark_temp.append(grade)\n",
        "        grade_writer.writerow(mark_temp)"
      ],
      "metadata": {
        "id": "dDgHkoqD3SG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/grade_data.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3BpiAdgVZx2",
        "outputId": "5079c264-0b59-4239-a12f-b36886247066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            HW1         HW2         HW3         HW4    Project  Presentation  \\\n",
            "0     36.249140   21.124081  100.000000   63.508417  35.197116     51.120449   \n",
            "1    100.000000  100.000000    0.000000   91.190683  45.595355     75.199435   \n",
            "2    100.000000  100.000000  100.000000   54.632752  65.274875    100.000000   \n",
            "3     34.898110    0.000000    0.000000   95.265725  68.939036     42.646325   \n",
            "4     42.065471  100.000000    0.000000   63.812843  92.026085      0.000000   \n",
            "..          ...         ...         ...         ...        ...           ...   \n",
            "995   81.435463   76.115736   97.771826   37.634132   0.000000     91.815616   \n",
            "996   55.649738    0.000000  100.000000   62.647968   1.452038     88.330421   \n",
            "997    0.000000   57.878650   84.270310   72.206063  36.808877    100.000000   \n",
            "998   34.257229   82.156567   48.827749  100.000000  73.022961    100.000000   \n",
            "999  100.000000   76.830479  100.000000    0.000000  70.822358      0.000000   \n",
            "\n",
            "           Exam  Grade  \n",
            "0     27.334922      0  \n",
            "1     15.031709      1  \n",
            "2      0.000000      1  \n",
            "3      0.000000      0  \n",
            "4    100.000000      2  \n",
            "..          ...    ...  \n",
            "995   63.719325      1  \n",
            "996  100.000000      2  \n",
            "997   57.170471      2  \n",
            "998   21.805614      2  \n",
            "999  100.000000      2  \n",
            "\n",
            "[1000 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Layer"
      ],
      "metadata": {
        "id": "pJnmu4UZZqzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the representation of the hypothesis, we define <span style=\"border-bottom: dashed\">sigmoid function</span> :\n",
        "$$\n",
        "h_{\\theta}=\\frac{1}{1+e^{-\\theta^Tx}}\n",
        "$$\n",
        "Note that $\\theta^Tx$ can be non-linear. \n",
        "    <br> <span style=\"border-bottom: dashed\">Cost function</span> in logistic refression is defined as: \n",
        "$$\n",
        "J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m-ylog(h_\\theta)-(1-y)log(1-h_\\theta),\n",
        "$$\n",
        "and still, we can use gradient descent to minimize it.\n",
        "    \n",
        "<br>In addition, we use <span style=\"border-bottom: dashed\">accuracy function</span> to see how well the algorithm works, which is defined as: \n",
        "$$\n",
        "Accuracy=\\frac{1}{m}\\sum_{i=1}^{m}\\delta_{predicted,real}\n",
        "$$"
      ],
      "metadata": {
        "id": "rFmoJRWoZqzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import fmin_tnc\n",
        "\n",
        "\n",
        "class LogisticRegressionUsingGD:\n",
        "\n",
        "# Activation function used to map any real value between 0 and 1\n",
        "    @staticmethod\n",
        "    def sigmoid(x):  \n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Computes the weighted sum of inputs Similar to Linear Regression\n",
        "    @staticmethod\n",
        "    def net_input(theta, x):\n",
        "        return np.dot(x, theta)\n",
        "\n",
        "# Calculates the probability that an instance belongs to a particular class\n",
        "    def probability(self, theta, x):\n",
        "        return self.sigmoid(self.net_input(theta, x))\n",
        "\n",
        "# Computes the cost function for all the training samples\n",
        "    def cost_function(self, theta, x, y):\n",
        "        m = x.shape[0]\n",
        "        total_cost = -(1 / m) * np.sum([np.log(self.probability(theta, x[i])) if y[i]==1 else np.log(1-self.probability(theta, x[i])) for i in range(m)])\n",
        "        return total_cost\n",
        "\n",
        "# Computes the gradient of the cost function at the point theta\n",
        "    def gradient(self, theta, x, y):     \n",
        "        m = x.shape[0]\n",
        "        return (1 / m) * np.dot(x.T, self.sigmoid(self.net_input(theta, x)) - y)\n",
        "\n",
        "    def fit(self, x, y, theta):\n",
        "        opt_weights = fmin_tnc(func=self.cost_function, x0=theta, fprime=self.gradient, args=(x, y.flatten()))\n",
        "        self.w_ = opt_weights[0]\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        theta = self.w_[:, np.newaxis]\n",
        "        return self.probability(theta, x)\n",
        "\n",
        "    def accuracy(self, x, actual_classes, probab_threshold=0.5):\n",
        "        predicted_classes = (self.predict(x) >= probab_threshold).astype(int)\n",
        "        predicted_classes = predicted_classes.flatten()\n",
        "        accuracy = np.mean(predicted_classes == actual_classes)\n",
        "        return accuracy * 100"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "mCkza3L7ZqzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we pick the features and outcome out of the initial data."
      ],
      "metadata": {
        "id": "x-mbv2xXZqzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df\n",
        "X = data.iloc[:, :7]        #features\n",
        "grade = data.iloc[:, 7]         #outcome\n",
        "X = np.c_[np.ones((X.shape[0], 1)), X]    #we need x_0"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "_KU7RTmqZqzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input whether or not each sample is in or above B range.  Assign 0 to all of those below B range, and vice versa. This devides the sample into two different classes for us to perform logistic regression."
      ],
      "metadata": {
        "id": "a5N_0sa0moMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cut_line=3\n",
        "y = [g>=cut_line for g in grade]                 #devide sample by whether they are in or above B(3) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X.shape[1], 1))"
      ],
      "metadata": {
        "id": "c0yOEN7MlrZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Logistic Regression algorithm."
      ],
      "metadata": {
        "id": "t9oxGFVXZqzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = LogisticRegressionUsingGD()\n",
        "model1.fit(X, y, theta)\n",
        "accuracy = model1.accuracy(X, y.flatten())\n",
        "parameters = model1.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "tTiuBiMYZqzJ",
        "outputId": "9e570c2a-191b-414b-d67e-22fbdb3a5f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-349e78460cf8>:10: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 100.0\n",
            "The model parameters got by Gradient descent:\n",
            "[-8.97079318e+02  8.46116947e-01  9.34451800e-01  8.53017027e-01\n",
            "  8.60690527e-01  2.45366453e+00  2.40635755e+00  3.58143347e+00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-349e78460cf8>:24: RuntimeWarning: divide by zero encountered in log\n",
            "  total_cost = -(1 / m) * np.sum([np.log(self.probability(theta, x[i])) if y[i]==1 else np.log(1-self.probability(theta, x[i])) for i in range(m)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We normalize the parameters ($\\theta$) by letting the sum of weight (excluding bias term) equal to 1, as it should be.  We can see the 7 weights we used to determine the grade of each sample and bias term being negative of cut line."
      ],
      "metadata": {
        "id": "AMr8k5gseecn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])\n",
        "print(\"The cut line for\", letter_grade[cut_line], \"range is {}\".format(-parameters_normalized[0]))\n",
        "print(\"The weights of the 7 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fMEQqB4dm9B",
        "outputId": "742f1bfd-9ab5-4a1d-add8-dd021fb682f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for B range is 75.15913809654148\n",
            "The weights of the 7 components are [0.07088941 0.07829028 0.07146751 0.07211041 0.20557303 0.20160955\n",
            " 0.30005981]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can try the same by deviding in C range and reach the same result, except for the bias term"
      ],
      "metadata": {
        "id": "xRg05s9HhrVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cut_line=2\n",
        "y = [g>=cut_line for g in grade]                 #devide sample by whether they are in or above C(2) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X.shape[1], 1))"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PM5rbmBahp88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LogisticRegressionUsingGD()\n",
        "model2.fit(X, y, theta)\n",
        "accuracy = model2.accuracy(X, y.flatten())\n",
        "parameters = model2.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "6fcbe51e-380a-4443-8b97-68dcaad04ba3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqEaNYDwhp89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-349e78460cf8>:10: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 100.0\n",
            "The model parameters got by Gradient descent:\n",
            "[-1491.81164564     1.84531356     1.86653083     1.92945695\n",
            "     1.84191534     4.99497452     4.9016153      7.48664545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])\n",
        "print(\"The cut line for\", letter_grade[cut_line], \"range is {}\".format(-parameters_normalized[0]))\n",
        "print(\"The weights of the 7 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb080a8-d847-4909-d07d-ff2f90fd7a53",
        "id": "qXQHs3ryljmb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for C range is 59.992943438272754\n",
            "The weights of the 7 components are [0.07420896 0.07506221 0.07759277 0.0740723  0.20087202 0.1971176\n",
            " 0.30107413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the case where we cannot know about all relevant input. Is it still possible to get the result using remaining data.\n",
        "\n",
        "In the following case we ignore the effect of the first two assignments which takes up 15% combined."
      ],
      "metadata": {
        "id": "JXZ2-NWZxGg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = data.iloc[:, 2:7]        #features (excluding first 2 columns)\n",
        "grade = data.iloc[:, 7]         #outcome\n",
        "X2 = np.c_[np.ones((X2.shape[0], 1)), X2]    #we need x_0"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nErzDFcVw72e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_line=3\n",
        "y = [g>=cut_line for g in grade]                 #devide sample by whether they are in or above B(3) range\n",
        "y = np.array(y)\n",
        "theta = np.zeros((X2.shape[1], 1))"
      ],
      "metadata": {
        "id": "ioNtJtOmw72f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = LogisticRegressionUsingGD()\n",
        "model3.fit(X2, y, theta)\n",
        "accuracy = model3.accuracy(X2, y.flatten())\n",
        "parameters = model3.w_\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "912db724-c278-435f-8ea8-b81bf5a7ee64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMVDWHZ7w72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 96.5\n",
            "The model parameters got by Gradient descent:\n",
            "[-34.4141174    0.04138935   0.04085416   0.10492733   0.10122372\n",
            "   0.15178631]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time we normalize the weights sum up to only 85%, but rescale the bias term up so that it is still out of 100 points.\n",
        "\n",
        "Note that we can still get a close result for the remaining components with slightly less accuracy."
      ],
      "metadata": {
        "id": "oDR-naEUxyc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_normalized=parameters/sum(parameters[1:])*0.85\n",
        "print(\"The cut line for\", letter_grade[cut_line], \"range is {}\".format(-parameters_normalized[0]/0.85))\n",
        "print(\"The weights of the remaining 5 components are {}\".format(parameters_normalized[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615e0d26-5d87-46a7-e0e5-11b8f6b14c0d",
        "id": "trDzfh9tw72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cut line for B range is 78.18176601653337\n",
            "The weights of the remaining 5 components are [0.07992384 0.07889039 0.20261724 0.19546547 0.29310307]\n"
          ]
        }
      ]
    }
  ]
}